{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svKZZvpiz1QN",
        "outputId": "59c6d84a-2533-4367-9b7e-eb25b4b95c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fleiss' Kappa: 0.6666666666666665\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "\n",
        "# Load annotations from a single JSON file\n",
        "def load_annotations(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "# Combine annotations from multiple files\n",
        "def load_multiple_files(file_paths):\n",
        "    all_annotations = []\n",
        "    for file_path in file_paths:\n",
        "        annotations = load_annotations(file_path)\n",
        "        all_annotations.extend(annotations)\n",
        "    return all_annotations\n",
        "\n",
        "\n",
        "# Convert the annotations to a matrix of classifications\n",
        "def create_classification_matrix(annotations):\n",
        "    # Initialize a dictionary to hold counts of classifications for each image\n",
        "    image_classifications = defaultdict(list)\n",
        "\n",
        "    # Organize annotations by image ID\n",
        "    for annotation in annotations:\n",
        "        image_id = annotation['image'][-11:]\n",
        "        choice = annotation['choice']\n",
        "        image_classifications[image_id].append(choice)\n",
        "\n",
        "    # Create a matrix where each row represents an image and columns represent counts of \"Truck\" and \"No Trucks\"\n",
        "    matrix = []\n",
        "    for image_id, choices in image_classifications.items():\n",
        "        # Count how many chose \"Truck\" and how many chose \"No Trucks\"\n",
        "        count_truck = choices.count('Truck')+choices.count('Trucks')\n",
        "        count_no_truck = choices.count('No Trucks')+choices.count('No Truck')\n",
        "\n",
        "        # Add the row to the matrix\n",
        "        matrix.append([count_truck, count_no_truck])\n",
        "\n",
        "    return np.array(matrix)\n",
        "\n",
        "\n",
        "# Compute Fleiss' Kappa\n",
        "def compute_fleiss_kappa(matrix):\n",
        "    # Statsmodels' Fleiss' Kappa function expects each row to represent an item (image)\n",
        "    # and each column to represent the count of raters who selected each category.\n",
        "    kappa = fleiss_kappa(matrix)\n",
        "    return kappa\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main(file_paths):\n",
        "    annotations = load_multiple_files(file_paths)\n",
        "    matrix = create_classification_matrix(annotations)\n",
        "    kappa = compute_fleiss_kappa(matrix)\n",
        "    print(f\"Fleiss' Kappa: {kappa}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_paths = ['cv1_v1.json', 'cv_2.json', 'cv3.json']  # Replace with your actual file paths\n",
        "    main(file_paths)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Helper function to create a mapping from text to labels\n",
        "def create_label_mapping(annotation):\n",
        "    label_mapping = defaultdict(list)\n",
        "    # Access the 'label' key, which should contain a list of dictionaries\n",
        "    for item in annotation['label']:\n",
        "        # Each item in this list is a dictionary\n",
        "        label_mapping[item['text']].append(item['labels'][0])\n",
        "    return label_mapping\n",
        "\n",
        "# Function to calculate Cohen's Kappa between two annotations\n",
        "def calculate_cohen_kappa(annotator1_json, annotator2_json):\n",
        "    # Load JSON data\n",
        "    with open(annotator1_json, 'r', encoding='utf-8') as f:\n",
        "        annotator1_data = json.load(f)\n",
        "    with open(annotator2_json, 'r', encoding='utf-8') as f:\n",
        "        annotator2_data = json.load(f)\n",
        "\n",
        "    # Create label mappings for both annotations\n",
        "    label_mapping1 = create_label_mapping(annotator1_data[0]) # Pass the first dictionary in the list\n",
        "    label_mapping2 = create_label_mapping(annotator2_data[0]) # Pass the first dictionary in the list\n",
        "\n",
        "\n",
        "    # Get the union of all texts from both annotations\n",
        "    all_texts = set(label_mapping1.keys()).union(set(label_mapping2.keys()))\n",
        "\n",
        "    # Create label lists for Cohen's Kappa calculation\n",
        "    labels1 = []\n",
        "    labels2 = []\n",
        "\n",
        "    for text in all_texts:\n",
        "        labels1.append(label_mapping1.get(text, ['None'])[0])  # Default to 'None' if no label\n",
        "        labels2.append(label_mapping2.get(text, ['None'])[0])  # Default to 'None' if no label\n",
        "\n",
        "    # Calculate Cohen's Kappa\n",
        "    kappa = cohen_kappa_score(labels1, labels2)\n",
        "    return kappa\n",
        "\n",
        "# Example usage\n",
        "annotator1_json = '/content/NEPjson1.json'  # Replace with actual file path\n",
        "annotator2_json = '/content/nlp_v2.json'  # Replace with actual file path\n",
        "kappa_score = calculate_cohen_kappa(annotator1_json, annotator2_json)\n",
        "print(f'Cohen\\'s Kappa: {kappa_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSgPyZG6n6S7",
        "outputId": "3311f7b3-927f-42b1-b2ac-1d42657754f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.6285714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ytiVrkVorh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}